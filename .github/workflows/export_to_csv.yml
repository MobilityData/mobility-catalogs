name: Export catalogs to CSV

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9]
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v2
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest wheel numpy
          sudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable
          sudo apt-get update
          sudo apt-get install gdal-bin python3-gdal
          sudo apt-get install libgdal-dev
          pip install GDAL==$(gdal-config --version) --global-option=build_ext --global-option="-I/usr/include/gdal"
          sudo apt-get install libspatialindex-dev
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      - name: Export the catalog of sources as CSV
        uses: jannekem/run-python-script-action@v1
        with:
          script: |
            import pandas as pd
            import os
            import json

            CSV_PATH = "./catalog_sources.csv"
            CSV_COLUMNS = [
              'mdb_source_id',
              'data_type',
              'location.country_code',
              'location.subdivision_name',
              'location.municipality',
              'agency',
              'name',
              'static_reference',
              'urls.auto_discovery',
              'urls.latest',
              'urls.license',
              'urls.realtime_vehicle_positions',
              'urls.realtime_trip_updates',
              'urls.realtime_alerts',
              'location.bounding_box.minimum_latitude',
              'location.bounding_box.maximum_latitude',
              'location.bounding_box.minimum_longitude',
              'location.bounding_box.maximum_longitude',
              'location.bounding_box.extracted_on'
            ]

            # tools.constants
            GTFS = "gtfs"
            GTFS_RT = "gtfs_rt"
            MDB_SOURCE_ID = "mdb_source_id"
            DATA_TYPE = "data_type"
            LOCATION = "location"
            COUNTRY_CODE = "country_code"
            SUBDIVISION_NAME = "subdivision_name"
            MUNICIPALITY = "municipality"
            STATIC_REFERENCE = "static_reference"
            UNKNOWN = "unknown"

            # tools.constants.GTFS_SCHEDULE_CATALOG_PATH_FROM_ROOT
            GTFS_SCHEDULE_CATALOG_PATH_FROM_ROOT = "catalogs/sources/gtfs/schedule"

            # tools.constants.GTFS_REALTIME_CATALOG_PATH_FROM_ROOT
            GTFS_REALTIME_CATALOG_PATH_FROM_ROOT = "catalogs/sources/gtfs/realtime"

            # tools.operations.get_sources
            gtfs_schedule_catalog_path = os.path.join(".", GTFS_SCHEDULE_CATALOG_PATH_FROM_ROOT)
            gtfs_realtime_catalog_path = os.path.join(".", GTFS_REALTIME_CATALOG_PATH_FROM_ROOT)
            catalog = {}
            for catalog_path in [gtfs_schedule_catalog_path, gtfs_realtime_catalog_path]:
                for path, sub_dirs, files in os.walk(catalog_path):
                    for file in files:
                        with open(os.path.join(path, file)) as fp:
                            entity_json = json.load(fp)
                            entity_id = entity_json[MDB_SOURCE_ID]
                            catalog[entity_id] = entity_json
            # Complete the GTFS Realtime Sources with the location information from their static reference
            for source_id, source in catalog.items():
                if source.get(DATA_TYPE) == GTFS_RT:
                    if source.get(STATIC_REFERENCE) is not None and catalog.get(source.get(STATIC_REFERENCE), {}).get(LOCATION) is not None:
                        source[LOCATION] = catalog.get(source.get(STATIC_REFERENCE), {}).get(LOCATION)
                    else:
                        source[LOCATION] = {COUNTRY_CODE: UNKNOWN, SUBDIVISION_NAME: UNKNOWN, MUNICIPALITY: UNKNOWN}
                    catalog[source_id] = source
            # Sort the catalog and convert it to a list
            catalog = list(dict(sorted(catalog.items())).values())

            # tools.helpers.to_csv
            path = CSV_PATH
            columns = CSV_COLUMNS
            catalog = pd.json_normalize(catalog)
            if columns is not None:
                catalog = catalog[columns]
            if STATIC_REFERENCE in catalog:
                catalog[STATIC_REFERENCE] = catalog[STATIC_REFERENCE].astype('Int64')
            catalog.to_csv(path, sep=",", index=False)

      - name: Upload the catalog of sources CSV artifact
        uses: actions/upload-artifact@v1
        with:
          name: catalog-sources-csv-v${{ github.run_id }}.${{ github.run_number }}
          path: ./catalog_sources.csv